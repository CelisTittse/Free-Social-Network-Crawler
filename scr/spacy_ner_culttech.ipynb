{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y31f_UAGaTtX",
        "outputId": "af3899c2-8b35-498c-a338-6e90c282f7e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xx-ent-wiki-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-3.8.0/xx_ent_wiki_sm-3.8.0-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xx-ent-wiki-sm\n",
            "Successfully installed xx-ent-wiki-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('xx_ent_wiki_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python3 -m spacy download xx_ent_wiki_sm\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "def load_spacy_ner(model_s: str = \"xx_ent_wiki_sm\"):\n",
        "    return spacy.load(model_s, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
        "\n",
        "model_str = 'xx_ent_wiki_sm'\n",
        "ner = load_spacy_ner(model_s=model_str)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def find_word_in_url (url: str, search_list: list) -> bool:\n",
        "    for search_str in search_list:\n",
        "        if search_str in url:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# remove social media and music sites\n",
        "def filter_websites (data: list, socialMedia: list) -> list:\n",
        "    filt_data = []\n",
        "    for webpage in data:\n",
        "        url = webpage['url']\n",
        "\n",
        "        # if social media website not in url add to filtered results\n",
        "        if not find_word_in_url(url=url, search_list=socialMedia):\n",
        "            filt_data.append(webpage)\n",
        "    return filt_data\n",
        "\n",
        "def filter_on_name(data: list, nameDict: dict) -> list:\n",
        "    filt_data = []\n",
        "    for website in data:\n",
        "        # isolate e.g. \"12.1\" -> int(12)\n",
        "        id_name = int(website['key'].split('.')[0])\n",
        "\n",
        "        # retrieve search name\n",
        "        name = nameDict[id_name]\n",
        "\n",
        "        # if name in website text add to filtered results\n",
        "        if name.lower() in website['full_text'].lower():\n",
        "            website.update({'name': name})\n",
        "            filt_data.append(website)\n",
        "\n",
        "    return filt_data\n",
        "\n",
        "def post_process_names_d (results:list=False) -> list:\n",
        "      # remove social medias\n",
        "      results = filter_websites(data=results,\n",
        "                                socialMedia=social_medias)\n",
        "\n",
        "      # remove text that don't include name from searched person\n",
        "      results = filter_on_name (data=results,\n",
        "                                nameDict=gt_id_name_d)\n",
        "\n",
        "      return results\n",
        "\n"
      ],
      "metadata": {
        "id": "WQaGa3_wavKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_fulltext_names(text: str, nlp) -> list[dict]:\n",
        "  results = []\n",
        "  docs = [nlp(sent) for sent in text.split('.')]\n",
        "\n",
        "  for doc in docs:\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PER\" and spacy.explain(ent.label_) == \"Named person or family.\":\n",
        "            results.append({\n",
        "                'word': ent.text,\n",
        "                'label': ent.label_,\n",
        "            })\n",
        "\n",
        "  return results\n",
        "\n",
        "def sort_ner_results_to_df(raw_data: list, nlp) -> pd.DataFrame:\n",
        "    df_d = {'source_key': [],\n",
        "            'source_name': [],\n",
        "            'target_name': [],\n",
        "            'score': [],\n",
        "            'url': [],\n",
        "            }\n",
        "\n",
        "    for web_data in tqdm(raw_data):\n",
        "        url = web_data['url']\n",
        "        name = web_data['name']\n",
        "        full_text = web_data['full_text']\n",
        "        source_key = web_data['key']\n",
        "\n",
        "        ner_results = parse_fulltext_names(text=full_text, nlp=nlp)\n",
        "\n",
        "        for ner_result in ner_results:\n",
        "            if ner_result['word'] not in name and ' ' in ner_result['word']:\n",
        "                df_d['source_key'].append(source_key)\n",
        "                df_d['url'].append(str(url))\n",
        "                df_d['source_name'].append(name)\n",
        "                df_d['target_name'].append(ner_result['word'])\n",
        "                df_d['score'].append(None)\n",
        "\n",
        "    return pd.DataFrame(df_d)\n"
      ],
      "metadata": {
        "id": "z6QzudrpbDjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import choices\n",
        "\n",
        "path_web_jsonl = 'gtWebText2.jsonl'\n",
        "path_gt_df = 'gt_contacts.xlsx'\n",
        "\n",
        "gt_df = pd.read_excel(path_gt_df)\n",
        "gt_id_name_d = dict(zip(gt_df['id'], gt_df['name']))\n",
        "\n",
        "\n",
        "social_medias = ['marktplaats', 'spotify', 'tiktok', 'linkedin', 'facebook',\n",
        "                 'music.apple', 'youtube', 'soundcloud', 'play.google', 'imdb']\n",
        "\n",
        "# open scraped websites texts\n",
        "with open(path_web_jsonl, encoding='utf-8-sig') as json_file:\n",
        "    # open json file\n",
        "    results = [json.loads(json_str) for json_str in list(json_file)]\n",
        "\n",
        "# filter SM, music, and movies websites\n",
        "filtered_pages_l = post_process_names_d(results=results)\n",
        "\n",
        "test_pages_l = choices(filtered_pages_l, k=10)\n",
        "\n",
        "ner_df_spacy = sort_ner_results_to_df (raw_data=filtered_pages_l,\n",
        "                                       nlp=ner)\n",
        "\n",
        "print(type(ner_df_spacy))\n",
        "\n",
        "print(ner_df_spacy.head(3))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7stqwJ0dqOi",
        "outputId": "56ce9d94-ec68-4b9d-f020-33e14308183b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1091/1091 [06:37<00:00,  2.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "  source_key  source_name    target_name score  \\\n",
            "0       60.0  Tess Stepek  Subject Staff  None   \n",
            "1       60.0  Tess Stepek   Hugo Damstra  None   \n",
            "2       60.0  Tess Stepek  Amy Moerkerke  None   \n",
            "\n",
            "                                                 url  \n",
            "0  https://www.uu.nl/en/news/utrecht-students-rec...  \n",
            "1  https://www.uu.nl/en/news/utrecht-students-rec...  \n",
            "2  https://www.uu.nl/en/news/utrecht-students-rec...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE\n",
        "\n",
        "def clean_string(s):\n",
        "    if isinstance(s, str):\n",
        "        return ILLEGAL_CHARACTERS_RE.sub(\"\", s)\n",
        "    return s\n",
        "\n",
        "df_clean = ner_df_spacy.applymap(clean_string)\n",
        "df_clean.to_excel('ner_persons_spacy.xlsx', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeCQRYgsi83I",
        "outputId": "f3088024-8910-4c6e-c404-19493aa05c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-a3f5d6452396>:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df_clean = ner_df_spacy.applymap(clean_string)\n"
          ]
        }
      ]
    }
  ]
}